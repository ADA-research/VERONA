{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robustness Experiment Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows examples how to use the different components of ada-verona. \n",
    "If these experiments are executed in a non-notebook context, one can make use of the ExperimentRepostory class to \n",
    "create and organise experiments in a structured manner. However, because this notebook shall show just the components \n",
    "and their input / output it does not make use of the ExperimentRepository class.\n",
    "To see examples on how to use it, one can take a look at the example scripts in the scripts/ folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from autoverify.verifier import Nnenum\n",
    "from ada_verona.database.network import Network\n",
    "\n",
    "from ada_verona.analysis.report_creator import ReportCreator\n",
    "from ada_verona.database.dataset.image_file_dataset import ImageFileDataset\n",
    "from ada_verona.database.dataset.pytorch_experiment_dataset import PytorchExperimentDataset\n",
    "from ada_verona.database.verification_context import VerificationContext\n",
    "from ada_verona.dataset_sampler.predictions_based_sampler import PredictionsBasedSampler\n",
    "from ada_verona.epsilon_value_estimator.binary_search_epsilon_value_estimator import (\n",
    "    BinarySearchEpsilonValueEstimator,\n",
    ")\n",
    "from ada_verona.verification_module.auto_verify_module import (\n",
    "    AutoVerifyModule,\n",
    ")\n",
    "from ada_verona.verification_module.property_generator.one2any_property_generator import (\n",
    "    One2AnyPropertyGenerator,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "torch.manual_seed(0)\n",
    "logging.basicConfig(format=\"%(asctime)s %(levelname)s %(message)s\", level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pytorch dataset. Preprocessing can be defined in the transform parameter\n",
    "torch_dataset = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# wrap pytorch dataset into experiment dataset to keep track of image id\n",
    "experiment_dataset = PytorchExperimentDataset(dataset=torch_dataset)\n",
    "\n",
    "# work on subset of the dataset to keep experiment small\n",
    "experiment_dataset = experiment_dataset.get_subset([x for x in range(0, 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, one can also use a custom dataset from the storage. \n",
    "# For this, one can make use of the ImageFileDataset class\n",
    "\n",
    "# Here, one can also add a preprocessing. \n",
    "# However, as of now just the loading of torch tensors from the directory is supported\n",
    "preprocessing = transform = torchvision.transforms.Compose([torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "custom_experiment_dataset = ImageFileDataset(\n",
    "    image_folder=Path(\"../tests/test_experiment/data/images\"),\n",
    "    label_file=Path(\"../tests/test_experiment/data/image_labels.csv\"),\n",
    "    preprocessing=preprocessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Component Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define verifier\n",
    "timeout = 300\n",
    "\n",
    "# In this example, a one to any property generator is used. \n",
    "# That creates vnnlib files for one to any robustness queries\n",
    "# A one to one property generator is also already implemented in the package and could be used here as well\n",
    "# For the property generator, we have to define the number of classes, \n",
    "# the lower bound of the data and the upper bound of the data\n",
    "property_generator = One2AnyPropertyGenerator(number_classes=10, data_lb=0, data_ub=10)\n",
    "\n",
    "# In this example, Nnenum is used. \n",
    "# All the other verifiers offered by the autoverify package can be used too in the AutoVerifyModule\n",
    "verifier = AutoVerifyModule(verifier=Nnenum(), timeout=timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute critical epsilon values, one can use the BinaySearchEpsilonValueEstimator class\n",
    "epsilon_value_list = [0.001, 0.1, 0.2, 0.3, 0.4]\n",
    "epsilon_value_estimator = BinarySearchEpsilonValueEstimator(epsilon_value_list=epsilon_value_list, verifier=verifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example we take one of the test networks\n",
    "network = Network(Path(\"../tests/test_experiment/data/networks/mnist-net_256x2.onnx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the robustness of a network, one first has\n",
    "# to check which data points are classified correctly.\n",
    "# For that the PredictionsBasedSampler class is used\n",
    "dataset_sampler = PredictionsBasedSampler(sample_correct_predictions=True)\n",
    "\n",
    "# Here all the data points that are correctly predicted by the network are sampled\n",
    "sampled_data = dataset_sampler.sample(network, experiment_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the 10 images in the sub dataset are predicted correctly by the network\n",
    "print(f\"Size of sampled dataset: {len(sampled_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Robustness Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute a critical epsilon values, for a given network and datapoint,\n",
    "# a verification context is created.\n",
    "# Also a folder for intermediate results needs to be provided to the VerificationContext,\n",
    "# so the vnnlib files can be stored there.\n",
    "# In addition, the results of the epsilon values queries can be stored there\n",
    "results = []\n",
    "now = datetime.now()\n",
    "now_string = now.strftime(\"%d-%m-%Y+%H_%M\")\n",
    "\n",
    "# Here the intermediate results (the per epsilon queries )\n",
    "intermediate_result_base_path = Path(f\"intermediate_results/{now_string}\")\n",
    "\n",
    "for data_point in sampled_data:\n",
    "    network_name = network.path.name.split(\".\")[0]\n",
    "    intermediate_result_path = Path(intermediate_result_base_path / f\"{network_name}/image_{data_point.id}\")\n",
    "\n",
    "    verification_context = VerificationContext(\n",
    "        network,\n",
    "        data_point,\n",
    "        intermediate_result_path,\n",
    "        property_generator=property_generator,\n",
    "    )\n",
    "    epsilon_value_result = epsilon_value_estimator.compute_epsilon_value(verification_context)\n",
    "\n",
    "    print(f\"result: {epsilon_value_result}\")\n",
    "    results.append(epsilon_value_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dicts = [x.to_dict() for x in results]\n",
    "result_df = pd.DataFrame(result_dicts)\n",
    "result_df[\"network\"] = (\n",
    "    result_df.network_path.astype(str).str.split(\"/\").apply(lambda x: x[-1]).apply(lambda x: x.split(\".\")[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_creator = ReportCreator(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_creator.create_box_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_creator.create_ecdf_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_creator.create_hist_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_creator.create_anneplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
