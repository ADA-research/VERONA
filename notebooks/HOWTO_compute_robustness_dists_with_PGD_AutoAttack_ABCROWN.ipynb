{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d01b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ada_verona import (\n",
    "    AttackEstimationModule,\n",
    "    AutoAttackWrapper,\n",
    "    BinarySearchEpsilonValueEstimator,\n",
    "    ExperimentRepository,\n",
    "    ImageFileDataset,\n",
    "    Network,\n",
    "    One2AnyPropertyGenerator,\n",
    "    PGDAttack,\n",
    "    PredictionsBasedSampler,\n",
    "    PytorchExperimentDataset,\n",
    "    VerificationContext,\n",
    ")\n",
    "from autoverify.verifier import AbCrown\n",
    "from robustness_experiment_box.verification_module.auto_verify_module import AutoVerifyModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed9ed1",
   "metadata": {},
   "source": [
    "## Guide on how to compute upper bounds to Robustness Distributions using adversarial attacks with ada-verona. \n",
    "\n",
    "The notebook shows how to use the different components of [VERONA](https://github.com/ADA-research/VERONA) and the corresponding [ada-verona](https://pypi.org/project/ada-verona/) package for setting up robustness experiments and computing robustness distributions of neural networks [1,2]. \n",
    "\n",
    "\n",
    "We'll cover:\n",
    "- Loading Models and Datasets with ada-verona\n",
    "- Running PGD and AutoAttack experiments\n",
    "- Running verification experiments with a formal verifier (AB-CROWN)\n",
    "\n",
    "---\n",
    "## 0. Setup\n",
    "\n",
    "Make sure you have installed the ada-verona package from PyPI: `uv pip install ada-verona`. If you need further instructions, please refer to the [VERONA README](https://github.com/ADA-research/VERONA/blob/main/README.md).\n",
    "\n",
    "To get started, you need a trained model and a dataset (e.g., MNIST).\n",
    "\n",
    "## 1. Loading Models and Datasets with VERONA\n",
    "\n",
    "The VERONA package is designed to make it easy for researchers to load models and datasets for robustness experiments.\n",
    "\n",
    "## Supported Model Format\n",
    "- **ONNX**: VERONA expects neural network models in the ONNX format (`.onnx` files).\n",
    "- You can convert PyTorch or TensorFlow models to ONNX using their respective export utilities. \n",
    "- Note: We plan to add direct support for torch models soon (3. Quarter of 2025).\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a93240e",
   "metadata": {},
   "source": [
    "### Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your ONNX model\n",
    "model_path = Path(\"data/MNIST/raw/models/mnist-net_256x2.onnx\")\n",
    "network = Network(model_path)\n",
    "\n",
    "# To load as a PyTorch model (for attacks, etc.)\n",
    "torch_model = network.load_pytorch_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954ceb7",
   "metadata": {},
   "source": [
    "### Loading a Dataset\n",
    "- For standard datasets (like MNIST), use the provided wrappers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eacb09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f98b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pytorch dataset. Preprocessing can be defined in the transform parameter\n",
    "torch_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "# wrap pytorch dataset into experiment dataset to keep track of image id\n",
    "experiment_dataset = PytorchExperimentDataset(dataset=torch_dataset)\n",
    "\n",
    "# work on subset of the dataset to keep experiment small\n",
    "experiment_dataset = experiment_dataset.get_subset([x for x in range(0, 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a9db08",
   "metadata": {},
   "source": [
    "- For custom datasets (e.g., images on disk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2788eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = Path(\"path/to/images\")\n",
    "label_file = Path(\"path/to/labels.csv\")\n",
    "custom_dataset = ImageFileDataset(image_folder=image_folder, label_file=label_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78982b5",
   "metadata": {},
   "source": [
    "### Sampling and Experiment Context\n",
    "- Use the `ExperimentRepository` to manage experiments and create verification contexts. In this example, a one to any property generator is used that creates vnnlib files for one to any robustness queries. A one to one property generator is also already implemented in the package and could be used here as well. For the property generator, we have to define the number of classes, the lower bound of the data and the upper bound of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101ef980",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_repository = ExperimentRepository(base_path=Path(\"experiments/\"), network_folder=Path(\"models/\"))\n",
    "network = experiment_repository.get_network_list()[0]\n",
    "data_point = experiment_dataset[0]\n",
    "property_generator = One2AnyPropertyGenerator()\n",
    "verification_context = experiment_repository.create_verification_context(network, data_point, property_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ead885",
   "metadata": {},
   "source": [
    "**For more details, see the [VERONA wiki](https://deepwiki.com/ADA-research/VERONA).** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265e62d",
   "metadata": {},
   "source": [
    "## 2. Run PGD and AutoAttack Experiments \n",
    "\n",
    "Below is a minimal example for running PGD [3] and AutoAttack-based [4,5] robustness estimation.\n",
    "\n",
    "- **Tip:** For more examples see the scripts in the `scripts/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd943923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset and experiment repository\n",
    "torch.manual_seed(0)\n",
    "epsilon_list = [0.001, 0.005, 0.05, 0.08]\n",
    "experiment_repository_path = Path(\"../tests/test_experiment\")\n",
    "network_folder = Path(\"data/MNIST/raw/models\")\n",
    "\n",
    "torch_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "dataset = PytorchExperimentDataset(dataset=torch_dataset)\n",
    "experiment_repository = ExperimentRepository(base_path=experiment_repository_path, network_folder=network_folder)\n",
    "property_generator = One2AnyPropertyGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac23c9",
   "metadata": {},
   "source": [
    "To compute the robustness of a network, one first has to check which data points are classified correctly. For that the PredictionsBasedSampler class is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e110a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sampler = PredictionsBasedSampler(sample_correct_predictions=True)\n",
    "\n",
    "# Here all the data points that are correctly predicted by the network are sampled\n",
    "sampled_data = dataset_sampler.sample(network, experiment_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf56b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD Attack\n",
    "verifier_pgd = AttackEstimationModule(attack=PGDAttack(number_iterations=10, step_size=0.01))\n",
    "epsilon_value_estimator_pgd = BinarySearchEpsilonValueEstimator(\n",
    "    epsilon_value_list=epsilon_list.copy(), verifier=verifier_pgd\n",
    ")\n",
    "\n",
    "# AutoAttack\n",
    "verifier_autoattack = AttackEstimationModule(attack=AutoAttackWrapper())\n",
    "epsilon_value_estimator_auto = BinarySearchEpsilonValueEstimator(\n",
    "    epsilon_value_list=epsilon_list.copy(), verifier=verifier_autoattack\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d2a6d",
   "metadata": {},
   "source": [
    "We use the pgd \"verifier\" `verifier_pgd ` from here on as an example for the next cells for illustrative purposes. Note that the verifier is used as a parameter to the Epsilon Estimator `epsilon_value_estimator_pgd`. The epsilon value estimator determines which search strategy is used to find adversarial examples (or, more precisely, estimate the interval of perturbation magnitude where the classification outcome changes). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91273abd",
   "metadata": {},
   "source": [
    "### Run the experiment (example for PGD)\n",
    "\n",
    "Here is a minimal example for one network and one data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = experiment_repository.get_network_list()[0]\n",
    "data_point = dataset[0]\n",
    "verification_context = experiment_repository.create_verification_context(network, data_point, property_generator)\n",
    "result = epsilon_value_estimator_pgd.compute_epsilon_value(verification_context)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c1735",
   "metadata": {},
   "source": [
    "The slightly more elaborate example below computes critical epsilon values for a given network and datapoint and again defines a verification context. The folder for intermediate results needs to be provided to the VerificationContext, so the vnnlib files can be stored there. In addition, the results of the epsilon values queries can also be stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c85694",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "now = datetime.now()\n",
    "now_string = now.strftime(\"%d-%m-%Y+%H_%M\")\n",
    "\n",
    "# Here the intermediate results (the per epsilon queries )\n",
    "intermediate_result_base_path = Path(f\"intermediate_results/{now_string}\")\n",
    "\n",
    "for data_point in sampled_data:\n",
    "    network_name = network.path.name.split(\".\")[0]\n",
    "    intermediate_result_path = Path(intermediate_result_base_path / f\"{network_name}/image_{data_point.id}\")\n",
    "\n",
    "    verification_context = VerificationContext(\n",
    "        network,\n",
    "        data_point,\n",
    "        intermediate_result_path,\n",
    "        property_generator=property_generator,\n",
    "    )\n",
    "    epsilon_value_result = epsilon_value_estimator_pgd.compute_epsilon_value(verification_context)\n",
    "\n",
    "    print(f\"result: {epsilon_value_result}\")\n",
    "    results.append(epsilon_value_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c16310",
   "metadata": {},
   "source": [
    "## 3. Run AbCrown Verification Experiments\n",
    "\n",
    "**Note:** Make sure that you have installed the `autoverify` package: `uv pip install autov-verify==0.1.4`.\n",
    "\n",
    "Below is an example for running formal verification-based robustness estimation using the AbCrown verifier. This provides **lower bounds** to robustness distributions (as opposed to upper bounds from adversarial attacks).\n",
    "\n",
    "- [**AB-CROWN**](https://github.com/Verified-Intelligence/alpha-beta-CROWN): A state-of-the-art neural network verifier that can provide formal guarantees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506460f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up experiment repository for AbCrown verification\n",
    "experiment_name = \"abcrown_verification\"\n",
    "timeout = 600  # 10 minutes timeout per verification query\n",
    "experiment_repository_path = Path(\"../tests/test_experiment\")\n",
    "network_folder = Path(\"data/MNIST/raw/models\")\n",
    "\n",
    "# Initialize experiment repository\n",
    "file_database = ExperimentRepository(base_path=experiment_repository_path, network_folder=network_folder)\n",
    "file_database.initialize_new_experiment(experiment_name)\n",
    "\n",
    "# Save experiment configuration\n",
    "file_database.save_configuration(\n",
    "    dict(\n",
    "        experiment_name=experiment_name,\n",
    "        experiment_repository_path=str(experiment_repository_path),\n",
    "        network_folder=str(network_folder),\n",
    "        timeout=timeout,\n",
    "        epsilon_list=[str(x) for x in epsilon_list],\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up AbCrown verifier with AutoVerifyModule wrapper\n",
    "property_generator = One2AnyPropertyGenerator()\n",
    "verifier = AutoVerifyModule(verifier=AbCrown(), timeout=timeout)\n",
    "\n",
    "# Create epsilon value estimator for formal verification\n",
    "epsilon_value_estimator_abcrown = BinarySearchEpsilonValueEstimator(\n",
    "    epsilon_value_list=epsilon_list.copy(), verifier=verifier\n",
    ")\n",
    "\n",
    "# Set up dataset sampler to only work with correctly predicted samples\n",
    "dataset_sampler = PredictionsBasedSampler(sample_correct_predictions=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AbCrown verification experiment\n",
    "network_list = file_database.get_network_list()\n",
    "\n",
    "for network in network_list:\n",
    "    print(f\"Processing network: {network.path.name}\")\n",
    "    \n",
    "    # Sample correctly predicted data points\n",
    "    sampled_data = dataset_sampler.sample(network, dataset)\n",
    "    print(f\"Found {len(sampled_data)} correctly predicted samples\")\n",
    "    \n",
    "    for data_point in sampled_data:\n",
    "        # Create verification context for this network-data pair\n",
    "        verification_context = file_database.create_verification_context(\n",
    "            network, data_point, property_generator\n",
    "        )\n",
    "        \n",
    "        # Compute epsilon value using formal verification\n",
    "        epsilon_value_result = epsilon_value_estimator_abcrown.compute_epsilon_value(verification_context)\n",
    "        \n",
    "        # Save result to experiment repository\n",
    "        file_database.save_result(epsilon_value_result)\n",
    "        \n",
    "        print(f\"Result for image {data_point.id}: {epsilon_value_result}\")\n",
    "\n",
    "# Generate plots and save experiment results\n",
    "file_database.save_plots()\n",
    "print(\"AbCrown verification experiment completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ac057",
   "metadata": {},
   "source": [
    "#### If you have questions or want to contribute, open an issue or pull request on our [VERONA GitHub](https://github.com/ADA-research/VERONA/issues)! Please also refer to our [VERONA wiki](https://deepwiki.com/ADA-research/VERONA) and more examples in the `scripts/` folder. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc8438",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "[1] A. W. Bosman, H. H. Hoos, and J. N. van Rijn, “A Preliminary Study of Critical Robustness Distributions in Neural Network Verification,” Proceedings of the 6th workshop on formal methods for ML-enabled autonomous systems, 2023. Available: https://ada.liacs.leidenuniv.nl/papers/BosEtAl23.pdf\n",
    "\n",
    "\n",
    "[2] A. Berger, N. Eberhardt, A. W. Bosman, H. Duwe, J. N. van Rijn, and H. Hoos, “Empirical Analysis of Upper Bounds of Robustness Distributions using Adversarial Attacks,” in THE 19TH LEARNING AND IN℡LIGENT OPTIMIZATION CONFERENCE, Accessed: Jul. 15, 2025. [Online]. Available: https://openreview.net/forum?id=jsfqoRrsjy\n",
    "\n",
    "[3] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards Deep Learning Models Resistant to Adversarial Attacks,” presented at the International Conference on Learning Representations, Feb. 2018. Accessed: Dec. 20, 2024. [Online]. Available: https://openreview.net/forum?id=rJzIBfZAb\n",
    "\n",
    "\n",
    "[4] F. Croce and M. Hein, “Mind the box: l_1 -APGD for sparse adversarial attacks on image classifiers,” in International Conference on Machine Learning, PMLR, 2021, pp. 2201–2211. Accessed: Jul. 16, 2025. [Online]. Available: http://proceedings.mlr.press/v139/croce21a.html\n",
    "[5] F. Croce and M. Hein, “Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,” in International conference on machine learning, PMLR, 2020, pp. 2206–2216. Accessed: May 03, 2025. [Online]. Available: https://proceedings.mlr.press/v119/croce20b.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
